---
jupyter:
  jupytext:
    encoding: '# -*- coding: utf-8 -*-'
    formats: ipynb,md,py
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.2'
      jupytext_version: 1.8.0
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

<!-- #region slideshow={"slide_type": "slide"} hideCode=false hidePrompt=false -->
# Chapter 4 - Decision Trees

Paul E. Anderson
<!-- #endregion -->

```python slideshow={"slide_type": "skip"}
%load_ext autoreload
%autoreload 2

from pathlib import Path
home = str(Path.home()) # all other paths are relative to this path. change to something else if this is not the case on your system
```

<!-- #region slideshow={"slide_type": "subslide"} -->
## How would you communicate your decision about a typical Thursday night?

Let's all pretend this isn't a pandemic for a second, and you have a new roommate who is part of an exchange program from your favorite foreign city. They are a little confused about what to do on their first Thursday night in town, so they ask you for help. You talk to them for a while, but it becomes clear they really won't leave you alone until you diagram your decision making progress. Let's take 5-10 minutes and come up with a tree you can share with the class :)

Here are your features:
* Deadline? $\in$ {Urgent, Near, None}
* Lazy? $\in$ {Yes, No}
* Party? $\in$ {Yes, No party going on tonight}

You are trying to predict one of the following activities:
* Activity $\in$ {Party, TV, Pub, Study}
<!-- #endregion -->

<!-- #region slideshow={"slide_type": "subslide"} -->
### What if we needed a way to create a tree that generalized to many people?

Onto our decision tree algorithm!
<!-- #endregion -->

```python slideshow={"slide_type": "fragment"}
import pandas as pd
import numpy as np

df = pd.read_csv(f'{home}/csc-466-student/data/activity.csv')
df
```

<!-- #region slideshow={"slide_type": "subslide"} -->
### Let's play a game of 20 questions

You've got 20 questions and you are trying to quess what your opponent is thinking by asking questions. 

For each question, you are trying to ask something that gives you the most information given what you already know. 

It is common to ask things like "Is it an animal?" before asking "Is it a cat?"

We need a way to encode this mathematically, and this leads us to information theory.
<!-- #endregion -->

<!-- #region slideshow={"slide_type": "subslide"} -->
We will reserve the writing of the decision tree algorithm to lab this week. Instead, we will focus on defining the process mathematically. To do this, we need to start discussing sets and metrics we can use about sets.
<!-- #endregion -->

### Entropy

We will first discuss the entropy of the universe. If said universe consisted of a set of items. 

Let $S$ be a set of items. Each element of $S$ can take on one of $Y$ values. We will write the unique values of $Y$ as $V_Y$.

$p(y)$ proportion of the number of elements in class ${\displaystyle y}$ to the number of elements in set

${\displaystyle \mathrm {H} {(S)}=\sum _{y\in V_Y}{-p(y)\log _{2}p(y)}}$

<!-- #region slideshow={"slide_type": "subslide"} -->
Let's look at our concrete example:
<!-- #endregion -->

```python slideshow={"slide_type": "fragment"}
S = df['Activity'].values
S
```

```python slideshow={"slide_type": "fragment"}
Y = df['Activity'].unique()
Y
```

<!-- #region slideshow={"slide_type": "subslide"} -->
**Stop and think:** How do you factor in prior knowledge (i.e., answers to prior questions)?
<!-- #endregion -->

<!-- #region slideshow={"slide_type": "fragment"} -->
Your solution here
<!-- #endregion -->

<!-- #region slideshow={"slide_type": "subslide"} -->
**Stop and think:** How do you measure the entropy if you knew all the possible answers?
<!-- #endregion -->

<!-- #region slideshow={"slide_type": "fragment"} -->
Your solution here
<!-- #endregion -->

<!-- #region slideshow={"slide_type": "subslide"} -->
### Now onto the ID3 algorithm!
From Marsland Chapter 12:


* If all examples have the same label:
    * return a leaf with that label
* Else if there are no features left to test:
    * return a leaf with the most common label
* Else:
    * choose the feature $F$ that maximises the information gain of $S$ to be the next node using Equation (12.2) Ë†
    * add a branch from the node for each possible value f in $F$
    * for each branch:
        * calculate $S_f$ by removing $F$ from the set of features
        * recursively call the algorithm with $S_f$ , to compute the gain relative to the current set of examples
<!-- #endregion -->

<!-- #region slideshow={"slide_type": "subslide"} -->
### Let's make our tree!

To the whiteboard!
<!-- #endregion -->

<!-- #region slideshow={"slide_type": "subslide"} -->
Now that we have a basic algorithm, what are some of the advances:
* Continuous variables
* C4.5 with pruning
<!-- #endregion -->

<!-- #region slideshow={"slide_type": "subslide"} -->
**Stop and think:** How would you adapt for continuous variables?
<!-- #endregion -->

<!-- #region slideshow={"slide_type": "subslide"} -->
So our next natural question is ... how do we compare algorithms?

##### Your solution here
<!-- #endregion -->

```python

```
